---
layout: post
title: 'NLP'
date: 2020-02-10
author: DoHerasYang
color: rgb(255,102,32)
cover: ''
tags: Notes

---

# Natural Language Processing



> This blog is based on the Course COM 6513 Natural Language Processing at the University of Sheffielld. 
>
> **@All the konowledge rights are reserved by the owner of this course's materials and Nikolaos Aletras and his team at the University of Sheffield.**

### 0. Assignment

#### 0.1 An example of unigrams, bigrams, trigrams 

![An-example-of-unigrams-bigrams-trigrams-and-4-grams-extracted-from-the-clinical-phrase](/Pictures/NLP/An-example-of-unigrams-bigrams-trigrams-and-4-grams-extracted-from-the-clinical-phrase.png)

N-grams of texts are extensively used in text mining and natural language processing tasks. An n-gram is a contiguous sequence of *n* items from a given sample of text or speech. an *n*-gram of **size 1 is referred to as a "unigram"**; **size 2 is a "bigram"**; **size 3 is a "trigram"**. When N>3 this is usually referred to as four grams or five grams and so on.

The formula to calculate the **Ngramk** to give the count of words:

+ Ngramk = X - (N -1)						X = The number of words in sentence / N = The number of n-grams 

For example:

>  Sentence:    I love NLP course at the University of Sheffield

For unigram:

​	The number of words you have to count:     Ngramk = 9	N = 1	Ngramk = 9-(1-1) = 9

​	So the unigram is [ I ] [ love ] [NLP] [ course ] [ at ] [ the ] [ University ] [ of ] [ Sheffield ]

For bigram:

​	The number of words you have to count:     Ngramk = 9	N = 2	Ngramk = 9-(2-1) = 8

​	so the bigram is [ I love ] [NLP course] [at the] [University of] [of Sheffield] [love NLP] [course at] [the University]	

and so on...







### 1.Introduction

#### 1.1 Word-Word Matrix

**Term-Context**

+ A matrix X, n × m where n = |V| (target words) and m = |Vc| (context words)
+ For each x from matrix X, which count how many times the V(target words) it co-occurs with context words.
+ Usually target and context word vocabularies are the same resulting into a square matrix.

![word-context](/Pictures/NLP/word-context.png)

#### 1.2 Document-Word Matrix (Bag-of-Words )

+ A matrix X, |D| × |V| where rows are documents in corpus D, and columns are vocabulary words in V.
+ X can also be obtained by adding all the one-hot vectors of the words in the documents and then transpose.

**one-hot vectors**

+ In [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing), a one-hot vector is a 1 × *N* matrix (vector) used to distinguish each word in a vocabulary from every other word in the vocabulary. The vector consists of 0s in all cells with the exception of a single 1 in a cell used uniquely to identify the word.

![doc-word](/Pictures/NLP/doc-word.png)

**Distance Discount**

Weight contexts according to the distance from the word: the further away, the lower the weight.

![distance discount](/Pictures/NLP/distance discount.png)

**Pointwise Mutual Information**

![PMI](/Pictures/NLP/PMI.png)

The PMI of a pair of outcomes *x* and *y* belonging to [discrete random variables](https://en.wikipedia.org/wiki/Discrete_random_variable) *X* and *Y* quantifies the discrepancy between the probability of their coincidence given their [joint distribution](https://en.wikipedia.org/wiki/Joint_distribution) and their individual distributions, assuming [independence](https://en.wikipedia.org/wiki/Statistical_independence)

![PMI(1)](/Pictures/NLP/PMI(1).png)

#### 1.3 Truncated Singular Value Decompostion

If the word-word PPMI matrix is very large and very difficult to handle with because of the complexity of the word matrix, this approach can reduce the dimensionality of the linear matrix.

![SVD](/Pictures/NLP/SVD.png)

For more details, you can click [here](http://langvillea.people.cofc.edu/DISSECTION-LAB/Emmie'sLSI-SVDModule/p5module.html).





