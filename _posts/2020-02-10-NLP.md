---
layout: post
title: 'NLP'
date: 2020-02-10
author: DoHerasYang
color: rgb(255,102,32)
cover: ''
tags: Notes

---

# Natural Language Processing



> This blog is based on the Course COM 6513 Natural Language Processing at the University of Sheffielld. 
>
> **@All the konowledge rights are reserved by the owner of this course's materials and Nikolaos Aletras and his team at the University of Sheffield.**

### 0.Assignment

#### 0.1 An example of unigrams, bigrams, trigrams and 4-grams 

![An-example-of-unigrams-bigrams-trigrams-and-4-grams-extracted-from-the-clinical-phrase](/Pictures/NLP/An-example-of-unigrams-bigrams-trigrams-and-4-grams-extracted-from-the-clinical-phrase.png)

N-grams of texts are extensively used in text mining and natural language processing tasks. An n-gram is a contiguous sequence of *n* items from a given sample of text or speech. an *n*-gram of **size 1 is referred to as a "unigram"**; **size 2 is a "bigram"**; **size 3 is a "trigram"**. When N>3 this is usually referred to as four grams or five grams and so on.

The formula to calculate the **Ngramk** to give the count of words:

+ Ngramk = X - (N -1)						X = The number of words in sentence / N = The number of n-grams 

For example:

>  Sentence:    I love NLP course at the University of Sheffield

For unigram:

​	The number of words you have to count:     Ngramk = 9	N = 1	Ngramk = 9-(1-1) = 9

​	So the unigram is [ I ] [ love ] [NLP] [ course ] [ at ] [ the ] [ University ] [ of ] [ Sheffield ]

For bigram:

​	The number of words you have to count:     Ngramk = 9	N = 2	Ngramk = 9-(2-1) = 8

​	so the bigram is [ I love ] [NLP course] [at the] [University of] [of Sheffield] [love NLP] [course at] [the University]	

and so on...











### 1.Introduction

